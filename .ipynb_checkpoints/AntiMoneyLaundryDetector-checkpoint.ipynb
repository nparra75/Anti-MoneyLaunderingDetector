{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ocIZcdD87xN"
      },
      "source": [
        "# Anti-Money Laundering Detector Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiGQpM0987xQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, sum, when, count, avg, to_timestamp, hour, dayofweek\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.sql import Window\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml.stat import Correlation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QdWyThH87xX",
        "outputId": "09e559c6-da5f-4172-eb0d-3ed3d423e431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icm4NiDA87xZ"
      },
      "outputs": [],
      "source": [
        "#os.environ['KAGGLE_CONFIG_DIR'] = '/dbfs/FileStore'\n",
        "#dbutils.fs.mkdirs(\"/FileStore/kaggle_data\")\n",
        "#!kaggle datasets download -d ealtman2019/ibm-transactions-for-anti-money-laundering-aml -p /dbfs/FileStore/kaggle_data --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jxlVmHYses6"
      },
      "outputs": [],
      "source": [
        "#(Replace Databrick) - Replace dbutils.fs.mkdirs with os.makedirs\n",
        "os.makedirs(\"/FileStore/kaggle_data\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEH-8oIZses7"
      },
      "outputs": [],
      "source": [
        "# (Replace Databrick)\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = './kaggle_config'  # Local directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw6uUnA4ses8",
        "outputId": "10eb21c6-704b-46ee-e5fa-cb0f2411a581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# (Replace Databrick) -\n",
        "os.makedirs(\"./kaggle_data\", exist_ok=True)  # Create a local directory\n",
        "# Download the dataset from kaggle\n",
        "os.system(\"kaggle datasets download -d ealtman2019/ibm-transactions-for-anti-money-laundering-aml -p ./kaggle_data --force\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76PEZpep87xb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def extract_csv_file(zip_path, file_name, extract_path, final_dbfs_path):\n",
        "    # Extract the specific file from the zip\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        if file_name in z.namelist():\n",
        "            z.extract(file_name, \"/databricks/driver/\")  # Extract to local driver\n",
        "            print(f\"File extracted to /databricks/driver/{file_name}\")\n",
        "\n",
        "            # Move the file to DBFS for Spark to read\n",
        "            dbutils.fs.cp(f\"file:/databricks/driver/{file_name}\", final_dbfs_path)\n",
        "            print(f\"File moved to {final_dbfs_path}\")\n",
        "\n",
        "            # Define schema for the transactions\n",
        "            schema = StructType([\n",
        "                StructField(\"Timestamp\", StringType(), True),\n",
        "                StructField(\"From_Bank\", StringType(), True),\n",
        "                StructField(\"From_Account\", StringType(), True),\n",
        "                StructField(\"To_Bank\", StringType(), True),\n",
        "                StructField(\"To_Account\", StringType(), True),\n",
        "                StructField(\"Amount_Received\", FloatType(), True),\n",
        "                StructField(\"Receiving_Currency\", StringType(), True),\n",
        "                StructField(\"Amount_Paid\", FloatType(), True),\n",
        "                StructField(\"Payment_Currency\", StringType(), True),\n",
        "                StructField(\"Payment_Format\", StringType(), True)\n",
        "            ])\n",
        "\n",
        "            # Read and display the file\n",
        "            df = spark.read.csv(final_dbfs_path,schema=schema, header=True)\n",
        "        else:\n",
        "            print(f\"{file_name} not found in the zip file.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnJ6kd42ses-"
      },
      "outputs": [],
      "source": [
        "#(Replace Databrick) - Extract the zip file and move it into output file\n",
        "def extract_csv_file_v1(zip_path, file_name, extract_path, final_local_path):\n",
        "    # Extract the specific file from the zip\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        if file_name in z.namelist():\n",
        "            # Extract the file to a local path\n",
        "            extracted_file_path = os.path.join(extract_path, file_name)\n",
        "            z.extract(file_name, extract_path)\n",
        "            print(f\"File extracted to {extracted_file_path}\")\n",
        "\n",
        "            # Move the file to the final local path (if needed)\n",
        "            os.rename(extracted_file_path, final_local_path)\n",
        "            print(f\"File moved to {final_local_path}\")\n",
        "\n",
        "            # Define schema for the transactions\n",
        "            schema = StructType([\n",
        "                StructField(\"Timestamp\", StringType(), True),\n",
        "                StructField(\"From_Bank\", StringType(), True),\n",
        "                StructField(\"From_Account\", StringType(), True),\n",
        "                StructField(\"To_Bank\", StringType(), True),\n",
        "                StructField(\"To_Account\", StringType(), True),\n",
        "                StructField(\"Amount_Received\", FloatType(), True),\n",
        "                StructField(\"Receiving_Currency\", StringType(), True),\n",
        "                StructField(\"Amount_Paid\", FloatType(), True),\n",
        "                StructField(\"Payment_Currency\", StringType(), True),\n",
        "                StructField(\"Payment_Format\", StringType(), True)\n",
        "            ])\n",
        "\n",
        "            # Initialize Spark session (if not already initialized)\n",
        "            spark = SparkSession.builder.appName(\"LocalCSVReader\").getOrCreate()\n",
        "\n",
        "            # Read and display the file\n",
        "            df = spark.read.csv(final_local_path, schema=schema, header=True)\n",
        "            df.show()\n",
        "        else:\n",
        "            print(f\"{file_name} not found in the zip file.\")\n",
        "            df = None\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1jGQV6Vses_"
      },
      "outputs": [],
      "source": [
        "# (Replace Databrick) - This version replace v1 to not extact again the file if it exists\n",
        "import zipfile\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "\n",
        "def extract_csv_file(zip_path, file_name, extract_path, final_local_path):\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(final_local_path):\n",
        "        print(f\"File already exists at {final_local_path}. Skipping extraction.\")\n",
        "    else:\n",
        "        # Extract the specific file from the zip\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            if file_name in z.namelist():\n",
        "                # Extract the file to a local path\n",
        "                extracted_file_path = os.path.join(extract_path, file_name)\n",
        "                z.extract(file_name, extract_path)\n",
        "                print(f\"File extracted to {extracted_file_path}\")\n",
        "\n",
        "                # Move the file to the final local path (if needed)\n",
        "                os.rename(extracted_file_path, final_local_path)\n",
        "                print(f\"File moved to {final_local_path}\")\n",
        "            else:\n",
        "                print(f\"{file_name} not found in the zip file.\")\n",
        "                return None\n",
        "\n",
        "    # Define schema for the transactions\n",
        "    schema = StructType([\n",
        "        StructField(\"Timestamp\", StringType(), True),\n",
        "        StructField(\"From_Bank\", StringType(), True),\n",
        "        StructField(\"From_Account\", StringType(), True),\n",
        "        StructField(\"To_Bank\", StringType(), True),\n",
        "        StructField(\"To_Account\", StringType(), True),\n",
        "        StructField(\"Amount_Received\", FloatType(), True),\n",
        "        StructField(\"Receiving_Currency\", StringType(), True),\n",
        "        StructField(\"Amount_Paid\", FloatType(), True),\n",
        "        StructField(\"Payment_Currency\", StringType(), True),\n",
        "        StructField(\"Payment_Format\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Initialize Spark session (if not already initialized)\n",
        "    spark = SparkSession.builder.appName(\"LocalCSVReader\").getOrCreate()\n",
        "\n",
        "    # Read the file into a DataFrame\n",
        "    df = spark.read.csv(final_local_path, schema=schema, header=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsbIbhGD87xb"
      },
      "outputs": [],
      "source": [
        "#hi_medium_df = extract_csv_file(\"/dbfs/FileStore/kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\", \"HI-Medium_Trans.csv\", \"/dbfs/FileStore/kaggle_data\", \"/FileStore/HI-Medium_Trans.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljo8HGU887xc"
      },
      "outputs": [],
      "source": [
        "#li_medium_df = extract_csv_file(\"/dbfs/FileStore/kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\", \"LI-Medium_Trans.csv\", \"/dbfs/FileStore/kaggle_data\", \"/FileStore/LI-Medium_Trans.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuXZvt12setA"
      },
      "outputs": [],
      "source": [
        "# (Replace Databrick) - Create local directories before extract the data\n",
        "import os\n",
        "os.makedirs(\"kaggle_data/extracted\", exist_ok=True)\n",
        "os.makedirs(\"output\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_kJ-G_ksetB",
        "outputId": "d57a0145-638f-401d-902f-4488ed872687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted to kaggle_data/extracted/HI-Medium_Trans.csv\n",
            "File moved to output/HI-Medium_Trans.csv\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick) - Update the paths for local use\n",
        "hi_medium_df = extract_csv_file(\n",
        "    zip_path=\"kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\",  # Local path to the zip file\n",
        "    file_name=\"HI-Medium_Trans.csv\",  # File name inside the zip\n",
        "    extract_path=\"kaggle_data/extracted\",  # Local directory for extraction\n",
        "    final_local_path=\"output/HI-Medium_Trans.csv\"  # Local path for the final CSV\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2vFu_GrsetB",
        "outputId": "d43242f6-109b-4e80-a55d-36eb615585ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted to kaggle_data/extracted/LI-Medium_Trans.csv\n",
            "File moved to output/LI-Medium_Trans.csv\n"
          ]
        }
      ],
      "source": [
        " # (Replace Databrick)\n",
        "li_medium_df = extract_csv_file(\n",
        "    zip_path=\"kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\",  # Local path to the zip file\n",
        "    file_name=\"LI-Medium_Trans.csv\",  # File name inside the zip\n",
        "    extract_path=\"kaggle_data/extracted\",  # Local directory for extraction\n",
        "    final_local_path=\"output/LI-Medium_Trans.csv\"  # Local path for the final CSV\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmVyGi2W87xd"
      },
      "source": [
        "### Combine HI-Medium and LI-Medium\n",
        "for real world scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6emES5CL87xe"
      },
      "outputs": [],
      "source": [
        "# Combine both datasets\n",
        "combined_df = hi_medium_df.union(li_medium_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31S9V4KC87xe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def extract_text(zip_path, file_name, extract_path, final_dbfs_path):\n",
        "    # Extract the specific file from the zip\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        if file_name in z.namelist():\n",
        "            z.extract(file_name, \"/databricks/driver/\")  # Extract to local driver\n",
        "            print(f\"File extracted to /databricks/driver/{file_name}\")\n",
        "\n",
        "            # Move the file to DBFS for Spark to read\n",
        "            dbutils.fs.cp(f\"file:/databricks/driver/{file_name}\", final_dbfs_path)\n",
        "            print(f\"File moved to {final_dbfs_path}\")\n",
        "\n",
        "            # Read and display the file\n",
        "            df = spark.read.text(final_dbfs_path)\n",
        "        else:\n",
        "            print(f\"{file_name} not found in the zip file.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqAoQFDjsetD"
      },
      "outputs": [],
      "source": [
        "#  - (replace Databricks)\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.appName(\"LocalTextReader\").getOrCreate()\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from pyspark.sql import SparkSession\n",
        "def extract_text_v1(zip_path, file_name, extract_path, final_local_path):\n",
        "    # Extract the specific file from the zip\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        if file_name in z.namelist():\n",
        "            # Extract the file to the specified local path\n",
        "            extracted_file_path = os.path.join(extract_path, file_name)\n",
        "            z.extract(file_name, extract_path)\n",
        "            print(f\"File extracted to {extracted_file_path}\")\n",
        "\n",
        "            # Move the file to the final local path (if needed)\n",
        "            os.rename(extracted_file_path, final_local_path)\n",
        "            print(f\"File moved to {final_local_path}\")\n",
        "\n",
        "            # Initialize Spark session (if not already initialized)\n",
        "            spark = SparkSession.builder.appName(\"LocalTextReader\").getOrCreate()\n",
        "\n",
        "            # Read and display the file using Spark\n",
        "            df = spark.read.text(final_local_path)\n",
        "            df.show()\n",
        "        else:\n",
        "            print(f\"{file_name} not found in the zip file.\")\n",
        "            df = None\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIVLdsgqsetD"
      },
      "outputs": [],
      "source": [
        "# (Replace Databrick) - Replace V1 to not extract the file if it already exists\n",
        "import os\n",
        "import zipfile\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session (if not already initialized)\n",
        "spark = SparkSession.builder.appName(\"LocalTextReader\").getOrCreate()\n",
        "\n",
        "def extract_text(zip_path, file_name, extract_path, final_local_path):\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(final_local_path):\n",
        "        print(f\"File already exists at {final_local_path}. Skipping extraction.\")\n",
        "    else:\n",
        "        # Extract the specific file from the zip\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            if file_name in z.namelist():\n",
        "                # Extract the file to the specified local path\n",
        "                extracted_file_path = os.path.join(extract_path, file_name)\n",
        "                z.extract(file_name, extract_path)\n",
        "                print(f\"File extracted to {extracted_file_path}\")\n",
        "\n",
        "                # Move the file to the final local path (if needed)\n",
        "                os.rename(extracted_file_path, final_local_path)\n",
        "                print(f\"File moved to {final_local_path}\")\n",
        "            else:\n",
        "                print(f\"{file_name} not found in the zip file.\")\n",
        "                return None\n",
        "\n",
        "    # Read the file into a Spark DataFrame\n",
        "    df = spark.read.text(final_local_path)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxaxltBD87xf"
      },
      "outputs": [],
      "source": [
        "# Load both pattern files\n",
        "#hi_patterns_df = extract_text(\"/dbfs/FileStore/kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\", \"HI-Medium_Patterns.txt\", \"/dbfs/FileStore/kaggle_data\", \"/FileStore/HI-Medium_Patterns.txt\")\n",
        "#li_patterns_df = extract_text(\"/dbfs/FileStore/kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\", \"LI-Medium_Patterns.txt\", \"/dbfs/FileStore/kaggle_data\", \"/FileStore/LI-Medium_Patterns.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGZD9G2asetE",
        "outputId": "e85dbed8-f495-423a-ff78-2c96822b8146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted to kaggle_data/extracted/HI-Medium_Patterns.txt\n",
            "File moved to output/HI-Medium_Patterns.txt\n",
            "File extracted to kaggle_data/extracted/LI-Medium_Patterns.txt\n",
            "File moved to output/LI-Medium_Patterns.txt\n"
          ]
        }
      ],
      "source": [
        "#(Replace Databrick) - Define paths for local use\n",
        "hi_patterns_df = extract_text(\n",
        "    zip_path=\"kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\",  # Local path to the zip file\n",
        "    file_name=\"HI-Medium_Patterns.txt\",  # File name inside the zip\n",
        "    extract_path=\"kaggle_data/extracted\",  # Local directory for extraction\n",
        "    final_local_path=\"output/HI-Medium_Patterns.txt\"  # Local path for the final extracted file\n",
        ")\n",
        "\n",
        "li_patterns_df = extract_text(\n",
        "    zip_path=\"kaggle_data/ibm-transactions-for-anti-money-laundering-aml.zip\",  # Local path to the zip file\n",
        "    file_name=\"LI-Medium_Patterns.txt\",  # File name inside the zip\n",
        "    extract_path=\"kaggle_data/extracted\",  # Local directory for extraction\n",
        "    final_local_path=\"output/LI-Medium_Patterns.txt\"  # Local path for the final extracted file\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abBakzE387xf"
      },
      "outputs": [],
      "source": [
        "# Union both pattern DataFrames\n",
        "patterns_df = hi_patterns_df.union(li_patterns_df)\n",
        "\n",
        "#patterns_df.cache().display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX7hzD-p87xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f15ce64-ff05-4ee9-8169-28c1aff2cca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|BEGIN LAUNDERING ...|\n",
            "|2022/09/01 05:14,...|\n",
            "|2022/09/03 13:09,...|\n",
            "|2022/09/01 07:40,...|\n",
            "|2022/09/01 14:19,...|\n",
            "|2022/09/02 12:40,...|\n",
            "|2022/09/03 06:34,...|\n",
            "|END LAUNDERING AT...|\n",
            "|                    |\n",
            "|BEGIN LAUNDERING ...|\n",
            "|2022/09/01 00:19,...|\n",
            "|2022/09/01 19:35,...|\n",
            "|2022/09/02 02:58,...|\n",
            "|2022/09/02 18:02,...|\n",
            "|2022/09/03 07:16,...|\n",
            "|2022/09/03 11:39,...|\n",
            "|2022/09/03 12:04,...|\n",
            "|2022/09/04 07:27,...|\n",
            "|2022/09/04 08:38,...|\n",
            "|2022/09/05 13:23,...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Number of rows in patterns_df: 36288\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick) - This replace Databricks display\n",
        "\n",
        "# Cache the DataFrame\n",
        "patterns_df.cache()\n",
        "\n",
        "# Show the data (use this in place of display)\n",
        "patterns_df.show()  # Displays the top rows of the DataFrame\n",
        "\n",
        "# Optionally count the rows to confirm caching works\n",
        "print(f\"Number of rows in patterns_df: {patterns_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esIuZFMu87xg"
      },
      "source": [
        "### Identify Laundering Patterns:\n",
        "Each laundering attempt begins with BEGIN LAUNDERING ATTEMPT - [PATTERN] and ends with END LAUNDERING ATTEMPT.\n",
        "\n",
        "Used regex to extract pattern types (e.g., Fan-Out, Cycle, Gather-Scatter, Stack) and transaction details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKSxJ-TR87xg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Step 1: Extract Pattern_Type where there is \"BEGIN LAUNDERING ATTEMPT\"\n",
        "patterns_df = patterns_df.withColumn(\n",
        "    \"Pattern_Type\",\n",
        "    F.when(F.col(\"value\").rlike(\"BEGIN LAUNDERING ATTEMPT - (.+)\"),\n",
        "           F.regexp_extract(F.col(\"value\"), \"BEGIN LAUNDERING ATTEMPT - (.+)\", 1))\n",
        "     .otherwise(None)\n",
        ")\n",
        "\n",
        "# Step 2: Forward fill the Pattern_Type to propagate it down until \"END LAUNDERING ATTEMPT\"\n",
        "window_spec = Window.orderBy(F.monotonically_increasing_id()).rowsBetween(Window.unboundedPreceding, 0)\n",
        "patterns_df = patterns_df.withColumn(\n",
        "    \"Pattern_Type\",\n",
        "    F.last(\"Pattern_Type\", True).over(window_spec)\n",
        ")\n",
        "\n",
        "# Step 3: Filter out rows with \"END LAUNDERING ATTEMPT\" as they only mark the end of an attempt\n",
        "patterns_df = patterns_df.filter(~F.col(\"value\").contains(\"END LAUNDERING ATTEMPT\"))\n",
        "\n",
        "\n",
        "patterns_df.display()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyHgpo2psetG"
      },
      "outputs": [],
      "source": [
        "# (Replace Databrick) - Correct the Databricks\n",
        "# Step 1: Extract Pattern_Type where there is \"BEGIN LAUNDERING ATTEMPT\"\n",
        "patterns_df = patterns_df.withColumn(\n",
        "    \"Pattern_Type\",\n",
        "    F.when(F.col(\"value\").rlike(\"BEGIN LAUNDERING ATTEMPT - (.+)\"),\n",
        "           F.regexp_extract(F.col(\"value\"), \"BEGIN LAUNDERING ATTEMPT - (.+)\", 1))\n",
        "     .otherwise(None)\n",
        ")\n",
        "\n",
        "# Step 2: Forward fill the Pattern_Type to propagate it down until \"END LAUNDERING ATTEMPT\"\n",
        "window_spec = Window.orderBy(F.monotonically_increasing_id()).rowsBetween(Window.unboundedPreceding, 0)\n",
        "patterns_df = patterns_df.withColumn(\n",
        "    \"Pattern_Type\",\n",
        "    F.last(\"Pattern_Type\", True).over(window_spec)\n",
        ")\n",
        "\n",
        "# Step 3: Filter out rows with \"END LAUNDERING ATTEMPT\" as they only mark the end of an attempt\n",
        "patterns_df = patterns_df.filter(~F.col(\"value\").contains(\"END LAUNDERING ATTEMPT\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njTKZdU4setG",
        "outputId": "87e4fbea-ab92-4bea-87fa-b89e15dc39f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------+-------------------+\n",
            "|value                                                                                           |Pattern_Type       |\n",
            "+------------------------------------------------------------------------------------------------+-------------------+\n",
            "|BEGIN LAUNDERING ATTEMPT - STACK                                                                |STACK              |\n",
            "|2022/09/01 05:14,00952,8139F54E0,0111632,8062C56E0,5331.44,US Dollar,5331.44,US Dollar,ACH,1    |STACK              |\n",
            "|2022/09/03 13:09,0111632,8062C56E0,008456,81363F620,5602.59,US Dollar,5602.59,US Dollar,ACH,1   |STACK              |\n",
            "|2022/09/01 07:40,0118693,823D5EB90,013729,801CF2E60,1400.54,US Dollar,1400.54,US Dollar,ACH,1   |STACK              |\n",
            "|2022/09/01 14:19,013729,801CF2E60,0123621,81A7090F0,1467.94,US Dollar,1467.94,US Dollar,ACH,1   |STACK              |\n",
            "|2022/09/02 12:40,0024750,81363F410,0213834,808757B00,16898.29,US Dollar,16898.29,US Dollar,ACH,1|STACK              |\n",
            "|2022/09/03 06:34,0213834,808757B00,000,800073EF0,17607.19,US Dollar,17607.19,US Dollar,ACH,1    |STACK              |\n",
            "|                                                                                                |STACK              |\n",
            "|BEGIN LAUNDERING ATTEMPT - CYCLE:  Max 12 hops                                                  |CYCLE:  Max 12 hops|\n",
            "|2022/09/01 00:19,0134266,814167590,0036925,810E343A0,132713.46,Yuan,132713.46,Yuan,ACH,1        |CYCLE:  Max 12 hops|\n",
            "|2022/09/01 19:35,0036925,810E343A0,0119211,814AB4F60,18264.20,US Dollar,18264.20,US Dollar,ACH,1|CYCLE:  Max 12 hops|\n",
            "|2022/09/02 02:58,0119211,814AB4F60,0132965,81B88A230,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE:  Max 12 hops|\n",
            "|2022/09/02 18:02,0132965,81B88A230,0137089,810C71940,114329.26,Yuan,114329.26,Yuan,ACH,1        |CYCLE:  Max 12 hops|\n",
            "|2022/09/03 07:16,0137089,810C71940,0216618,81D5302D0,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE:  Max 12 hops|\n",
            "|2022/09/03 11:39,0216618,81D5302D0,0024083,81836B520,13629.75,Euro,13629.75,Euro,ACH,1          |CYCLE:  Max 12 hops|\n",
            "|2022/09/03 12:04,0024083,81836B520,0038110,81B868730,97481.96,Yuan,97481.96,Yuan,ACH,1          |CYCLE:  Max 12 hops|\n",
            "|2022/09/04 07:27,0038110,81B868730,0225015,81C6EA460,14054.71,US Dollar,14054.71,US Dollar,ACH,1|CYCLE:  Max 12 hops|\n",
            "|2022/09/04 08:38,0225015,81C6EA460,018112,8045CC910,13718.22,US Dollar,13718.22,US Dollar,ACH,1 |CYCLE:  Max 12 hops|\n",
            "|2022/09/05 13:23,018112,8045CC910,007818,8037732C0,12908.33,US Dollar,12908.33,US Dollar,ACH,1  |CYCLE:  Max 12 hops|\n",
            "|2022/09/06 05:10,007818,8037732C0,0121523,80D1BD2F0,10636.75,Euro,10636.75,Euro,ACH,1           |CYCLE:  Max 12 hops|\n",
            "+------------------------------------------------------------------------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick) - Display the results\n",
        "patterns_df.show(truncate=False)  # Show the DataFrame without truncating long strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH0PWvk887xh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Remove any text after the colon in Pattern_Type if it exists\n",
        "\n",
        "patterns_df = patterns_df.withColumn(\n",
        "    \"Pattern_Type\",\n",
        "    F.regexp_replace(F.col(\"Pattern_Type\"), \":.*\", \"\")\n",
        ")\n",
        "\n",
        "patterns_df.display()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGxRyQ17setH",
        "outputId": "733625b5-2beb-4dfa-b213-61b3f1406bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------+------------+\n",
            "|value                                                                                           |Pattern_Type|\n",
            "+------------------------------------------------------------------------------------------------+------------+\n",
            "|BEGIN LAUNDERING ATTEMPT - STACK                                                                |STACK       |\n",
            "|2022/09/01 05:14,00952,8139F54E0,0111632,8062C56E0,5331.44,US Dollar,5331.44,US Dollar,ACH,1    |STACK       |\n",
            "|2022/09/03 13:09,0111632,8062C56E0,008456,81363F620,5602.59,US Dollar,5602.59,US Dollar,ACH,1   |STACK       |\n",
            "|2022/09/01 07:40,0118693,823D5EB90,013729,801CF2E60,1400.54,US Dollar,1400.54,US Dollar,ACH,1   |STACK       |\n",
            "|2022/09/01 14:19,013729,801CF2E60,0123621,81A7090F0,1467.94,US Dollar,1467.94,US Dollar,ACH,1   |STACK       |\n",
            "|2022/09/02 12:40,0024750,81363F410,0213834,808757B00,16898.29,US Dollar,16898.29,US Dollar,ACH,1|STACK       |\n",
            "|2022/09/03 06:34,0213834,808757B00,000,800073EF0,17607.19,US Dollar,17607.19,US Dollar,ACH,1    |STACK       |\n",
            "|                                                                                                |STACK       |\n",
            "|BEGIN LAUNDERING ATTEMPT - CYCLE:  Max 12 hops                                                  |CYCLE       |\n",
            "|2022/09/01 00:19,0134266,814167590,0036925,810E343A0,132713.46,Yuan,132713.46,Yuan,ACH,1        |CYCLE       |\n",
            "|2022/09/01 19:35,0036925,810E343A0,0119211,814AB4F60,18264.20,US Dollar,18264.20,US Dollar,ACH,1|CYCLE       |\n",
            "|2022/09/02 02:58,0119211,814AB4F60,0132965,81B88A230,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE       |\n",
            "|2022/09/02 18:02,0132965,81B88A230,0137089,810C71940,114329.26,Yuan,114329.26,Yuan,ACH,1        |CYCLE       |\n",
            "|2022/09/03 07:16,0137089,810C71940,0216618,81D5302D0,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE       |\n",
            "|2022/09/03 11:39,0216618,81D5302D0,0024083,81836B520,13629.75,Euro,13629.75,Euro,ACH,1          |CYCLE       |\n",
            "|2022/09/03 12:04,0024083,81836B520,0038110,81B868730,97481.96,Yuan,97481.96,Yuan,ACH,1          |CYCLE       |\n",
            "|2022/09/04 07:27,0038110,81B868730,0225015,81C6EA460,14054.71,US Dollar,14054.71,US Dollar,ACH,1|CYCLE       |\n",
            "|2022/09/04 08:38,0225015,81C6EA460,018112,8045CC910,13718.22,US Dollar,13718.22,US Dollar,ACH,1 |CYCLE       |\n",
            "|2022/09/05 13:23,018112,8045CC910,007818,8037732C0,12908.33,US Dollar,12908.33,US Dollar,ACH,1  |CYCLE       |\n",
            "|2022/09/06 05:10,007818,8037732C0,0121523,80D1BD2F0,10636.75,Euro,10636.75,Euro,ACH,1           |CYCLE       |\n",
            "+------------------------------------------------------------------------------------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Replace Databrick\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Remove any text after the colon in Pattern_Type if it exists\n",
        "patterns_df = patterns_df.withColumn(\n",
        "    \"Pattern_Type\",\n",
        "    F.regexp_replace(F.col(\"Pattern_Type\"), \":.*\", \"\")\n",
        ")\n",
        "\n",
        "# Display the DataFrame (use .show() in local PySpark)\n",
        "patterns_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IOSQewU87xi"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Filter to get only transaction lines and ignore start/end laundering attempt lines\n",
        "laundering_transactions = patterns_df.filter(patterns_df.value.rlike(r'\\d{4}/\\d{2}/\\d{2}'))\n",
        "patterns_df.unpersist()\n",
        "laundering_transactions.cache().display()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cluOpO4msetH",
        "outputId": "267233ce-27fc-4d2a-b55d-1b4f93e1584f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------+------------+\n",
            "|value                                                                                           |Pattern_Type|\n",
            "+------------------------------------------------------------------------------------------------+------------+\n",
            "|2022/09/01 05:14,00952,8139F54E0,0111632,8062C56E0,5331.44,US Dollar,5331.44,US Dollar,ACH,1    |STACK       |\n",
            "|2022/09/03 13:09,0111632,8062C56E0,008456,81363F620,5602.59,US Dollar,5602.59,US Dollar,ACH,1   |STACK       |\n",
            "|2022/09/01 07:40,0118693,823D5EB90,013729,801CF2E60,1400.54,US Dollar,1400.54,US Dollar,ACH,1   |STACK       |\n",
            "|2022/09/01 14:19,013729,801CF2E60,0123621,81A7090F0,1467.94,US Dollar,1467.94,US Dollar,ACH,1   |STACK       |\n",
            "|2022/09/02 12:40,0024750,81363F410,0213834,808757B00,16898.29,US Dollar,16898.29,US Dollar,ACH,1|STACK       |\n",
            "|2022/09/03 06:34,0213834,808757B00,000,800073EF0,17607.19,US Dollar,17607.19,US Dollar,ACH,1    |STACK       |\n",
            "|2022/09/01 00:19,0134266,814167590,0036925,810E343A0,132713.46,Yuan,132713.46,Yuan,ACH,1        |CYCLE       |\n",
            "|2022/09/01 19:35,0036925,810E343A0,0119211,814AB4F60,18264.20,US Dollar,18264.20,US Dollar,ACH,1|CYCLE       |\n",
            "|2022/09/02 02:58,0119211,814AB4F60,0132965,81B88A230,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE       |\n",
            "|2022/09/02 18:02,0132965,81B88A230,0137089,810C71940,114329.26,Yuan,114329.26,Yuan,ACH,1        |CYCLE       |\n",
            "|2022/09/03 07:16,0137089,810C71940,0216618,81D5302D0,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE       |\n",
            "|2022/09/03 11:39,0216618,81D5302D0,0024083,81836B520,13629.75,Euro,13629.75,Euro,ACH,1          |CYCLE       |\n",
            "|2022/09/03 12:04,0024083,81836B520,0038110,81B868730,97481.96,Yuan,97481.96,Yuan,ACH,1          |CYCLE       |\n",
            "|2022/09/04 07:27,0038110,81B868730,0225015,81C6EA460,14054.71,US Dollar,14054.71,US Dollar,ACH,1|CYCLE       |\n",
            "|2022/09/04 08:38,0225015,81C6EA460,018112,8045CC910,13718.22,US Dollar,13718.22,US Dollar,ACH,1 |CYCLE       |\n",
            "|2022/09/05 13:23,018112,8045CC910,007818,8037732C0,12908.33,US Dollar,12908.33,US Dollar,ACH,1  |CYCLE       |\n",
            "|2022/09/06 05:10,007818,8037732C0,0121523,80D1BD2F0,10636.75,Euro,10636.75,Euro,ACH,1           |CYCLE       |\n",
            "|2022/09/06 13:24,0121523,80D1BD2F0,0134266,814167590,1378736.88,Yen,1378736.88,Yen,ACH,1        |CYCLE       |\n",
            "|2022/09/01 00:25,0266915,81C3CD2E0,0266946,81A4AFE20,1155546.44,Ruble,1155546.44,Ruble,ACH,1    |FAN-IN      |\n",
            "|2022/09/01 03:18,0174634,81C5B2AB0,0266946,81A4AFE20,1104862.69,Ruble,1104862.69,Ruble,ACH,1    |FAN-IN      |\n",
            "+------------------------------------------------------------------------------------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick) - Filter to get only transaction lines and ignore start/end laundering attempt lines\n",
        "laundering_transactions = patterns_df.filter(patterns_df.value.rlike(r'\\d{4}/\\d{2}/\\d{2}'))\n",
        "\n",
        "# Unpersist patterns_df to free memory\n",
        "patterns_df.unpersist()\n",
        "\n",
        "# Cache laundering_transactions for reuse\n",
        "laundering_transactions.cache()\n",
        "\n",
        "# Display the filtered DataFrame (use .show() instead of display())\n",
        "laundering_transactions.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOkZRyi-87xj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Split the 'value' column into multiple columns\n",
        "laundering_transactions = laundering_transactions.withColumn(\"Timestamp\", F.split(F.col(\"value\"), \",\").getItem(0)) \\\n",
        "    .withColumn(\"From_Bank\", F.split(F.col(\"value\"), \",\").getItem(1)) \\\n",
        "    .withColumn(\"From_Account\", F.split(F.col(\"value\"), \",\").getItem(2)) \\\n",
        "    .withColumn(\"To_Bank\", F.split(F.col(\"value\"), \",\").getItem(3)) \\\n",
        "    .withColumn(\"To_Account\", F.split(F.col(\"value\"), \",\").getItem(4)) \\\n",
        "    .withColumn(\"Amount_Received\", F.split(F.col(\"value\"), \",\").getItem(5)) \\\n",
        "    .withColumn(\"Receiving_currency\", F.split(F.col(\"value\"), \",\").getItem(6)) \\\n",
        "    .withColumn(\"Amount_paid\", F.split(F.col(\"value\"), \",\").getItem(7)) \\\n",
        "    .withColumn(\"Payment_currency\", F.split(F.col(\"value\"), \",\").getItem(8)) \\\n",
        "    .withColumn(\"Payment_Format\", F.split(F.col(\"value\"), \",\").getItem(9)) \\\n",
        "    .withColumn(\"isLaundering\", F.split(F.col(\"value\"), \",\").getItem(10))\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyb7G-b2setI"
      },
      "outputs": [],
      "source": [
        "# (Replace Databrick) - Updated Code for Better Readability\n",
        "columns = [\n",
        "    \"Timestamp\", \"From_Bank\", \"From_Account\", \"To_Bank\", \"To_Account\",\n",
        "    \"Amount_Received\", \"Receiving_currency\", \"Amount_paid\",\n",
        "    \"Payment_currency\", \"Payment_Format\", \"isLaundering\"\n",
        "]\n",
        "\n",
        "for idx, col_name in enumerate(columns):\n",
        "    laundering_transactions = laundering_transactions.withColumn(col_name, F.split(F.col(\"value\"), \",\").getItem(idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0PFfihEsetJ",
        "outputId": "6fc1c08d-c87e-4ac0-f319-993132d0fbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------+------------+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+\n",
            "|value                                                                                           |Pattern_Type|Timestamp       |From_Bank|From_Account|To_Bank|To_Account|Amount_Received|Receiving_currency|Amount_paid|Payment_currency|Payment_Format|isLaundering|\n",
            "+------------------------------------------------------------------------------------------------+------------+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+\n",
            "|2022/09/01 05:14,00952,8139F54E0,0111632,8062C56E0,5331.44,US Dollar,5331.44,US Dollar,ACH,1    |STACK       |2022/09/01 05:14|00952    |8139F54E0   |0111632|8062C56E0 |5331.44        |US Dollar         |5331.44    |US Dollar       |ACH           |1           |\n",
            "|2022/09/03 13:09,0111632,8062C56E0,008456,81363F620,5602.59,US Dollar,5602.59,US Dollar,ACH,1   |STACK       |2022/09/03 13:09|0111632  |8062C56E0   |008456 |81363F620 |5602.59        |US Dollar         |5602.59    |US Dollar       |ACH           |1           |\n",
            "|2022/09/01 07:40,0118693,823D5EB90,013729,801CF2E60,1400.54,US Dollar,1400.54,US Dollar,ACH,1   |STACK       |2022/09/01 07:40|0118693  |823D5EB90   |013729 |801CF2E60 |1400.54        |US Dollar         |1400.54    |US Dollar       |ACH           |1           |\n",
            "|2022/09/01 14:19,013729,801CF2E60,0123621,81A7090F0,1467.94,US Dollar,1467.94,US Dollar,ACH,1   |STACK       |2022/09/01 14:19|013729   |801CF2E60   |0123621|81A7090F0 |1467.94        |US Dollar         |1467.94    |US Dollar       |ACH           |1           |\n",
            "|2022/09/02 12:40,0024750,81363F410,0213834,808757B00,16898.29,US Dollar,16898.29,US Dollar,ACH,1|STACK       |2022/09/02 12:40|0024750  |81363F410   |0213834|808757B00 |16898.29       |US Dollar         |16898.29   |US Dollar       |ACH           |1           |\n",
            "|2022/09/03 06:34,0213834,808757B00,000,800073EF0,17607.19,US Dollar,17607.19,US Dollar,ACH,1    |STACK       |2022/09/03 06:34|0213834  |808757B00   |000    |800073EF0 |17607.19       |US Dollar         |17607.19   |US Dollar       |ACH           |1           |\n",
            "|2022/09/01 00:19,0134266,814167590,0036925,810E343A0,132713.46,Yuan,132713.46,Yuan,ACH,1        |CYCLE       |2022/09/01 00:19|0134266  |814167590   |0036925|810E343A0 |132713.46      |Yuan              |132713.46  |Yuan            |ACH           |1           |\n",
            "|2022/09/01 19:35,0036925,810E343A0,0119211,814AB4F60,18264.20,US Dollar,18264.20,US Dollar,ACH,1|CYCLE       |2022/09/01 19:35|0036925  |810E343A0   |0119211|814AB4F60 |18264.20       |US Dollar         |18264.20   |US Dollar       |ACH           |1           |\n",
            "|2022/09/02 02:58,0119211,814AB4F60,0132965,81B88A230,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE       |2022/09/02 02:58|0119211  |814AB4F60   |0132965|81B88A230 |14567.69       |Euro              |14567.69   |Euro            |ACH           |1           |\n",
            "|2022/09/02 18:02,0132965,81B88A230,0137089,810C71940,114329.26,Yuan,114329.26,Yuan,ACH,1        |CYCLE       |2022/09/02 18:02|0132965  |81B88A230   |0137089|810C71940 |114329.26      |Yuan              |114329.26  |Yuan            |ACH           |1           |\n",
            "|2022/09/03 07:16,0137089,810C71940,0216618,81D5302D0,14567.69,Euro,14567.69,Euro,ACH,1          |CYCLE       |2022/09/03 07:16|0137089  |810C71940   |0216618|81D5302D0 |14567.69       |Euro              |14567.69   |Euro            |ACH           |1           |\n",
            "|2022/09/03 11:39,0216618,81D5302D0,0024083,81836B520,13629.75,Euro,13629.75,Euro,ACH,1          |CYCLE       |2022/09/03 11:39|0216618  |81D5302D0   |0024083|81836B520 |13629.75       |Euro              |13629.75   |Euro            |ACH           |1           |\n",
            "|2022/09/03 12:04,0024083,81836B520,0038110,81B868730,97481.96,Yuan,97481.96,Yuan,ACH,1          |CYCLE       |2022/09/03 12:04|0024083  |81836B520   |0038110|81B868730 |97481.96       |Yuan              |97481.96   |Yuan            |ACH           |1           |\n",
            "|2022/09/04 07:27,0038110,81B868730,0225015,81C6EA460,14054.71,US Dollar,14054.71,US Dollar,ACH,1|CYCLE       |2022/09/04 07:27|0038110  |81B868730   |0225015|81C6EA460 |14054.71       |US Dollar         |14054.71   |US Dollar       |ACH           |1           |\n",
            "|2022/09/04 08:38,0225015,81C6EA460,018112,8045CC910,13718.22,US Dollar,13718.22,US Dollar,ACH,1 |CYCLE       |2022/09/04 08:38|0225015  |81C6EA460   |018112 |8045CC910 |13718.22       |US Dollar         |13718.22   |US Dollar       |ACH           |1           |\n",
            "|2022/09/05 13:23,018112,8045CC910,007818,8037732C0,12908.33,US Dollar,12908.33,US Dollar,ACH,1  |CYCLE       |2022/09/05 13:23|018112   |8045CC910   |007818 |8037732C0 |12908.33       |US Dollar         |12908.33   |US Dollar       |ACH           |1           |\n",
            "|2022/09/06 05:10,007818,8037732C0,0121523,80D1BD2F0,10636.75,Euro,10636.75,Euro,ACH,1           |CYCLE       |2022/09/06 05:10|007818   |8037732C0   |0121523|80D1BD2F0 |10636.75       |Euro              |10636.75   |Euro            |ACH           |1           |\n",
            "|2022/09/06 13:24,0121523,80D1BD2F0,0134266,814167590,1378736.88,Yen,1378736.88,Yen,ACH,1        |CYCLE       |2022/09/06 13:24|0121523  |80D1BD2F0   |0134266|814167590 |1378736.88     |Yen               |1378736.88 |Yen             |ACH           |1           |\n",
            "|2022/09/01 00:25,0266915,81C3CD2E0,0266946,81A4AFE20,1155546.44,Ruble,1155546.44,Ruble,ACH,1    |FAN-IN      |2022/09/01 00:25|0266915  |81C3CD2E0   |0266946|81A4AFE20 |1155546.44     |Ruble             |1155546.44 |Ruble           |ACH           |1           |\n",
            "|2022/09/01 03:18,0174634,81C5B2AB0,0266946,81A4AFE20,1104862.69,Ruble,1104862.69,Ruble,ACH,1    |FAN-IN      |2022/09/01 03:18|0174634  |81C5B2AB0   |0266946|81A4AFE20 |1104862.69     |Ruble             |1104862.69 |Ruble           |ACH           |1           |\n",
            "+------------------------------------------------------------------------------------------------+------------+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick) - Display the results\n",
        "laundering_transactions.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMAUHP9-setK",
        "outputId": "81df1096-f4ca-4da6-d942-9dbddd626d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+------------+------------+\n",
            "|Timestamp       |From_Bank|Pattern_Type|isLaundering|\n",
            "+----------------+---------+------------+------------+\n",
            "|2022/09/01 05:14|00952    |STACK       |1           |\n",
            "|2022/09/03 13:09|0111632  |STACK       |1           |\n",
            "|2022/09/01 07:40|0118693  |STACK       |1           |\n",
            "|2022/09/01 14:19|013729   |STACK       |1           |\n",
            "|2022/09/02 12:40|0024750  |STACK       |1           |\n",
            "|2022/09/03 06:34|0213834  |STACK       |1           |\n",
            "|2022/09/01 00:19|0134266  |CYCLE       |1           |\n",
            "|2022/09/01 19:35|0036925  |CYCLE       |1           |\n",
            "|2022/09/02 02:58|0119211  |CYCLE       |1           |\n",
            "|2022/09/02 18:02|0132965  |CYCLE       |1           |\n",
            "|2022/09/03 07:16|0137089  |CYCLE       |1           |\n",
            "|2022/09/03 11:39|0216618  |CYCLE       |1           |\n",
            "|2022/09/03 12:04|0024083  |CYCLE       |1           |\n",
            "|2022/09/04 07:27|0038110  |CYCLE       |1           |\n",
            "|2022/09/04 08:38|0225015  |CYCLE       |1           |\n",
            "|2022/09/05 13:23|018112   |CYCLE       |1           |\n",
            "|2022/09/06 05:10|007818   |CYCLE       |1           |\n",
            "|2022/09/06 13:24|0121523  |CYCLE       |1           |\n",
            "|2022/09/01 00:25|0266915  |FAN-IN      |1           |\n",
            "|2022/09/01 03:18|0174634  |FAN-IN      |1           |\n",
            "+----------------+---------+------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "laundering_transactions = laundering_transactions.select(\"Timestamp\", \"From_Bank\", \"Pattern_Type\", \"isLaundering\")\n",
        "\n",
        "#  (Replace Databrick) - Display the results\n",
        "laundering_transactions.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZtGMmw387xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e2e7f4-03e8-44a4-a5fc-c79784ed92a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+------------+------------+\n",
            "|Timestamp|From_Bank|Pattern_Type|isLaundering|\n",
            "+---------+---------+------------+------------+\n",
            "|0        |0        |0           |0           |\n",
            "+---------+---------+------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count empty strings in each column\n",
        "empty_string_counts = laundering_transactions.select(\n",
        "    [sum(when(col(c) == \"\", 1).otherwise(0)).alias(c) for c in laundering_transactions.columns])\n",
        "\n",
        "# Show the result\n",
        "#empty_string_counts.show()\n",
        "\n",
        "# (Replace Databrick)\n",
        "# Show the result\n",
        "empty_string_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaqeZjGR87xk"
      },
      "outputs": [],
      "source": [
        "laundering_transactions.createOrReplaceTempView(\"combined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-p2VWUv87xk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "%sql\n",
        "\n",
        "SELECT Pattern_Type, COUNT(Pattern_Type) AS count FROM combined GROUP BY Pattern_Type\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HgQ6Gr3setU",
        "outputId": "55a6e91b-9969-4da6-c03a-1ec45119931a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|Pattern_Type  |count|\n",
            "+--------------+-----+\n",
            "|STACK         |4601 |\n",
            "|CYCLE         |2518 |\n",
            "|FAN-IN        |2644 |\n",
            "|GATHER-SCATTER|4830 |\n",
            "|BIPARTITE     |2623 |\n",
            "|FAN-OUT       |2617 |\n",
            "|SCATTER-GATHER|4874 |\n",
            "|RANDOM        |1945 |\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick)\n",
        "# Query using spark.sql()\n",
        "result_df = spark.sql(\"\"\"\n",
        "    SELECT Pattern_Type, COUNT(Pattern_Type) AS count\n",
        "    FROM combined\n",
        "    GROUP BY Pattern_Type\n",
        "\"\"\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YAWi4iR87xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd75992-72f8-433f-9717-6258aab536d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|isLaundering|count|\n",
            "+------------+-----+\n",
            "|           1|26652|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "laundering_transactions.cache().groupBy(\"isLaundering\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkrd17U87xl"
      },
      "source": [
        "### Create Labels for Laundering Transactions:\n",
        "Joined the laundering pattern DataFrame (laundering_transactions) with the combined transaction DataFrame (combined_df) on transaction identifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYWEz5je87xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62580587-e9b9-47ea-c6d8-3e151c23d3e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Timestamp: string, From_Bank: string, Pattern_Type: string, isLaundering: string]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Join the DataFrames on Timestamp, From_Bank, and To_Bank\n",
        "joined_df = combined_df.join(\n",
        "    laundering_transactions,\n",
        "    on=[\"Timestamp\", \"From_Bank\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "#freeing from cache\n",
        "combined_df.unpersist()\n",
        "laundering_transactions.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVNWBjJe87xm"
      },
      "outputs": [],
      "source": [
        "# Fill null values in the `isLaundering` column with 0\n",
        "joined_df = joined_df.withColumn(\n",
        "    \"isLaundering\",\n",
        "    F.when(F.col(\"isLaundering\").isNull(), F.lit(0)).otherwise(F.col(\"isLaundering\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL6r9HRA87xm"
      },
      "outputs": [],
      "source": [
        "#joined_df.cache().display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXEJ-Z8tsetW",
        "outputId": "31d691ca-432d-473e-fd06-63feeb1fa8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+\n",
            "|Timestamp       |From_Bank|From_Account|To_Bank|To_Account|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Pattern_Type|isLaundering|\n",
            "+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+\n",
            "|2022/09/01 00:17|020      |800104D70   |020    |800104D70 |6794.63        |US Dollar         |6794.63    |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:02|03196    |800107150   |03196  |800107150 |7739.29        |US Dollar         |7739.29    |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:17|01208    |80010E430   |01208  |80010E430 |1880.23        |US Dollar         |1880.23    |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:03|01208    |80010E650   |020    |80010E6F0 |7.396688E7     |US Dollar         |7.396688E7 |US Dollar       |Cheque        |NULL        |0           |\n",
            "|2022/09/01 00:02|01208    |80010E650   |020    |80010EA30 |4.5868456E7    |US Dollar         |4.5868456E7|US Dollar       |Cheque        |NULL        |0           |\n",
            "|2022/09/01 00:27|03203    |80010EA80   |03203  |80010EA80 |13284.41       |US Dollar         |13284.41   |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:25|020      |800104D20   |020    |800104D20 |9.72           |US Dollar         |9.72       |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:09|01208    |80010E430   |01208  |80010E430 |7.66           |US Dollar         |7.66       |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:09|011      |80010E600   |011    |80010E600 |16.33          |US Dollar         |16.33      |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:06|01208    |80010E650   |01208  |80010E650 |4.86           |US Dollar         |4.86       |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:25|020      |80010E6F0   |0183112|84DCA3150 |3.24           |US Dollar         |3.24       |US Dollar       |Credit Card   |NULL        |0           |\n",
            "|2022/09/01 00:16|020      |80010EA30   |01601  |802B6D220 |47.17          |US Dollar         |47.17      |US Dollar       |Credit Card   |NULL        |0           |\n",
            "|2022/09/01 00:14|020      |800073020   |020    |800073020 |848368.75      |US Dollar         |848368.75  |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:05|03566    |800345920   |03566  |800345920 |10134.05       |US Dollar         |10134.05   |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:09|011      |800329930   |02776  |800816450 |335999.7       |US Dollar         |335999.7   |US Dollar       |Cheque        |NULL        |0           |\n",
            "|2022/09/01 00:07|000      |8009B22F0   |000    |8009B22F0 |121.98         |US Dollar         |121.98     |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:24|011      |8003289F0   |0249457|825F4B630 |45.39          |US Dollar         |45.39      |US Dollar       |Credit Card   |NULL        |0           |\n",
            "|2022/09/01 00:22|000      |800815DE0   |000    |800815DE0 |23.4           |US Dollar         |23.4       |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "|2022/09/01 00:28|02776    |800816450   |0071901|81AB268F0 |30.23          |US Dollar         |30.23      |US Dollar       |Credit Card   |NULL        |0           |\n",
            "|2022/09/01 00:21|011081   |8008DDDF0   |011081 |8008DDDF0 |7.37           |US Dollar         |7.37       |US Dollar       |Reinvestment  |NULL        |0           |\n",
            "+----------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (- Replace Databrick) Cache the joined DataFrame\n",
        "joined_df.cache()\n",
        "\n",
        "# Display the cached DataFrame (use .show() instead of .display())\n",
        "joined_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gDE5U-F87xm",
        "outputId": "ab1ab959-6181-4dbc-84ec-0bdf9f5cb866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Timestamp: string (nullable = true)\n",
            " |-- From_Bank: string (nullable = true)\n",
            " |-- From_Account: string (nullable = true)\n",
            " |-- To_Bank: string (nullable = true)\n",
            " |-- To_Account: string (nullable = true)\n",
            " |-- Amount_Received: float (nullable = true)\n",
            " |-- Receiving_Currency: string (nullable = true)\n",
            " |-- Amount_Paid: float (nullable = true)\n",
            " |-- Payment_Currency: string (nullable = true)\n",
            " |-- Payment_Format: string (nullable = true)\n",
            " |-- Pattern_Type: string (nullable = true)\n",
            " |-- isLaundering: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert 'isLaundering' from string to integer\n",
        "joined_df = joined_df.withColumn(\"isLaundering\", col(\"isLaundering\").cast(\"integer\"))\n",
        "\n",
        "\n",
        "# Verify the schema change\n",
        "joined_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz6rq_D887xn"
      },
      "outputs": [],
      "source": [
        "#size_in_memory_gb = joined_df.rdd.map(lambda row: len(str(row))).sum() / (1024 * 1024 * 1024)\n",
        "#print(f\"Approximate size in memory: ({size_in_memory_gb:.2f} GB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRoRhJKh87xn",
        "outputId": "30f6db7f-55c0-4b28-f8ad-10e93b612a62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|isLaundering|   count|\n",
            "+------------+--------+\n",
            "|           1|   84928|\n",
            "|           0|63065007|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joined_df.groupBy(\"isLaundering\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7p3WoE087xn"
      },
      "source": [
        "### Balance the data\n",
        "DROP 50% of majority\n",
        "Use SMOTE to generate synthetic data of minority after data engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Ey1NSK87xo"
      },
      "outputs": [],
      "source": [
        "# Register joined_df as a temporary view to use SQL\n",
        "joined_df.createOrReplaceTempView(\"joined_table\")\n",
        "\n",
        "# Step 1: Identify rows to drop\n",
        "# Here we assign a random value to each row that meets the condition, then select 50% of those rows\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM joined_table\n",
        "WHERE NOT ((Pattern_Type IS NULL) AND (isLaundering = 0) AND (rand() < 0.5))\n",
        "\"\"\"\n",
        "\n",
        "# Execute the SQL query\n",
        "balanced_df = spark.sql(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7d4J9QF87xo",
        "outputId": "39024479-ad04-48d6-9b35-b543da0e4552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[Timestamp: string, From_Bank: string, From_Account: string, To_Bank: string, To_Account: string, Amount_Received: float, Receiving_Currency: string, Amount_Paid: float, Payment_Currency: string, Payment_Format: string, Pattern_Type: string, isLaundering: int]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joined_df.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu4RcrMs87xo",
        "outputId": "56cccc7b-6a25-487a-e02d-0c9d8b9bbab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|isLaundering|   count|\n",
            "+------------+--------+\n",
            "|           1|   84928|\n",
            "|           0|31531299|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "balanced_df.cache().groupBy(\"isLaundering\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgcntRHn87xo"
      },
      "source": [
        "### Data Cleaning and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmdFk3HP87xp",
        "outputId": "2ae2f010-30a6-41eb-deea-db76b45da605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+\n",
            "|Timestamp|From_Bank|From_Account|To_Bank|To_Account|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Pattern_Type|isLaundering|\n",
            "+---------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+\n",
            "|        0|        0|           0|      0|         0|              0|                 0|          0|               0|             0|    31531299|           0|\n",
            "+---------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count NULL values in each column\n",
        "null_counts = balanced_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in balanced_df.columns])\n",
        "\n",
        "# Show the result\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__0IrKfv87xx"
      },
      "outputs": [],
      "source": [
        "balanced_df = balanced_df.na.fill({\n",
        "    \"Pattern_Type\": \"Unknown\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "162KQ4xX87xy"
      },
      "source": [
        "Extract features such as Hour and DayOfWeek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNO7h5WC87xy"
      },
      "outputs": [],
      "source": [
        "balanced_df = balanced_df.withColumn(\"Timestamp\", to_timestamp(\"Timestamp\", \"yyyy/MM/dd HH:mm\")) \\\n",
        "                         .withColumn(\"Hour\", hour(\"Timestamp\")) \\\n",
        "                         .withColumn(\"DayOfWeek\", dayofweek(\"Timestamp\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPUCN_l687xz"
      },
      "outputs": [],
      "source": [
        "#balanced_df.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv23tFAgsetc",
        "outputId": "e5abd206-01a5-4ecd-9850-27845b1c55da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+\n",
            "|Timestamp          |From_Bank|From_Account|To_Bank|To_Account|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Pattern_Type|isLaundering|Hour|DayOfWeek|\n",
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+\n",
            "|2022-09-01 00:17:00|020      |800104D70   |020    |800104D70 |6794.63        |US Dollar         |6794.63    |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:17:00|01208    |80010E430   |01208  |80010E430 |1880.23        |US Dollar         |1880.23    |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:09:00|01208    |80010E430   |01208  |80010E430 |7.66           |US Dollar         |7.66       |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:06:00|01208    |80010E650   |01208  |80010E650 |4.86           |US Dollar         |4.86       |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:25:00|020      |80010E6F0   |0183112|84DCA3150 |3.24           |US Dollar         |3.24       |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:16:00|020      |80010EA30   |01601  |802B6D220 |47.17          |US Dollar         |47.17      |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:14:00|020      |800073020   |020    |800073020 |848368.75      |US Dollar         |848368.75  |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:05:00|03566    |800345920   |03566  |800345920 |10134.05       |US Dollar         |10134.05   |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:07:00|000      |8009B22F0   |000    |8009B22F0 |121.98         |US Dollar         |121.98     |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:24:00|011      |8003289F0   |0249457|825F4B630 |45.39          |US Dollar         |45.39      |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:22:00|000      |800815DE0   |000    |800815DE0 |23.4           |US Dollar         |23.4       |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:15:00|01601    |800970C80   |0260376|825F955A0 |121.38         |US Dollar         |121.38     |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:13:00|021418   |800A37E30   |001315 |8009BDC30 |178265.0       |US Dollar         |178265.0   |US Dollar       |Cheque        |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:20:00|001315   |8009BDC30   |0125106|816DF5400 |165.63         |US Dollar         |165.63     |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:00:00|001046   |800A37D90   |0274159|820C04F20 |26.42          |US Dollar         |26.42      |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:09:00|01601    |800A38370   |021418 |808FC8680 |0.09           |US Dollar         |0.09       |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:27:00|001488   |800AB1FB0   |02776  |800A3A8C0 |369786.0       |US Dollar         |369786.0   |US Dollar       |Cheque        |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:19:00|031857   |800AB37C0   |031857 |800AB37C0 |84.38          |US Dollar         |84.38      |US Dollar       |Reinvestment  |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:06:00|020      |800A39BD0   |004011 |8430C6190 |59.37          |US Dollar         |59.37      |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "|2022-09-01 00:10:00|02776    |800A3A8C0   |0261418|85127D560 |0.06           |US Dollar         |0.06       |US Dollar       |Credit Card   |Unknown     |0           |0   |5        |\n",
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (Replace Databrick) Show the final DataFrame (replacing .display() with .show())\n",
        "balanced_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upAKu18387xz"
      },
      "source": [
        "### Feature Engineering\n",
        "Aggregated Features by Account using Window Functions:\n",
        "Calculate FanOut, FanIn, and AvgAmountSent.\n",
        "\n",
        "-FanOut:\n",
        "how many different transactions it initiates.\n",
        "\n",
        "-FanIn:\n",
        "how many different transactions it receives.\n",
        "\n",
        "-AvgAmountSent:\n",
        "the typical transaction size for each account as a sender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig0M585v87xz"
      },
      "outputs": [],
      "source": [
        "# Window specifications\n",
        "sender_window = Window.partitionBy(\"From_Account\")\n",
        "receiver_window = Window.partitionBy(\"To_Account\")\n",
        "\n",
        "# Calculate fan-out, fan-in, and average amount sent\n",
        "featured_df = balanced_df.withColumn(\"FanOut\", count(\"To_Account\").over(sender_window)) \\\n",
        "                         .withColumn(\"FanIn\", count(\"From_Account\").over(receiver_window)) \\\n",
        "                         .withColumn(\"AvgAmountSent\", avg(\"Amount_Paid\").over(sender_window))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjUgk3so87xz",
        "outputId": "92f0e245-d180-4c85-a288-89fc79d25369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+------+-----+------------------+\n",
            "|Timestamp          |From_Bank|From_Account|To_Bank|To_Account|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Pattern_Type|isLaundering|Hour|DayOfWeek|FanOut|FanIn|AvgAmountSent     |\n",
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+------+-----+------------------+\n",
            "|2022-09-01 05:21:00|004      |800060A20   |004    |800060A20 |1834.72        |Rupee             |1834.72    |Rupee           |Reinvestment  |Unknown     |0           |5   |5        |234   |43   |208131.82313641932|\n",
            "|2022-09-01 19:24:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |19  |5        |234   |26   |208131.82313641932|\n",
            "|2022-09-01 05:46:00|004      |800060A20   |004    |800060A20 |10683.91       |Rupee             |10683.91   |Rupee           |Credit Card   |Unknown     |0           |5   |5        |234   |43   |208131.82313641932|\n",
            "|2022-09-01 19:09:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |19  |5        |234   |26   |208131.82313641932|\n",
            "|2022-09-01 05:53:00|004      |800060A20   |004    |800060A20 |11860.6        |Rupee             |11860.6    |Rupee           |Wire          |Unknown     |0           |5   |5        |234   |43   |208131.82313641932|\n",
            "|2022-09-06 01:02:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |1   |3        |234   |26   |208131.82313641932|\n",
            "|2022-09-15 15:17:00|004      |800060A20   |004    |800060A20 |10683.91       |Rupee             |10683.91   |Rupee           |Credit Card   |Unknown     |0           |15  |5        |234   |43   |208131.82313641932|\n",
            "|2022-09-06 01:25:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |1   |3        |234   |26   |208131.82313641932|\n",
            "|2022-09-02 07:04:00|004      |800060A20   |004    |800060A20 |21381.18       |Rupee             |21381.18   |Rupee           |Cheque        |Unknown     |0           |7   |6        |234   |43   |208131.82313641932|\n",
            "|2022-09-08 18:49:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |18  |5        |234   |26   |208131.82313641932|\n",
            "|2022-09-15 15:24:00|004      |800060A20   |004    |800060A20 |11860.6        |Rupee             |11860.6    |Rupee           |Wire          |Unknown     |0           |15  |5        |234   |43   |208131.82313641932|\n",
            "|2022-09-08 18:31:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |18  |5        |234   |26   |208131.82313641932|\n",
            "|2022-09-02 07:16:00|004      |800060A20   |004    |800060A20 |10683.91       |Rupee             |10683.91   |Rupee           |Credit Card   |Unknown     |0           |7   |6        |234   |43   |208131.82313641932|\n",
            "|2022-09-09 14:26:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |14  |6        |234   |26   |208131.82313641932|\n",
            "|2022-09-02 07:18:00|004      |800060A20   |004    |800060A20 |11860.6        |Rupee             |11860.6    |Rupee           |Wire          |Unknown     |0           |7   |6        |234   |43   |208131.82313641932|\n",
            "|2022-09-12 12:22:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |12  |2        |234   |26   |208131.82313641932|\n",
            "|2022-09-05 19:53:00|004      |800060A20   |004    |800060A20 |3117.27        |Euro              |268273.75  |Rupee           |ACH           |STACK       |1           |19  |2        |234   |43   |208131.82313641932|\n",
            "|2022-09-12 12:27:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |12  |2        |234   |26   |208131.82313641932|\n",
            "|2022-09-05 12:43:00|004      |800060A20   |004    |800060A20 |21381.18       |Rupee             |21381.18   |Rupee           |Cheque        |Unknown     |0           |12  |2        |234   |43   |208131.82313641932|\n",
            "|2022-09-13 05:06:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |5   |3        |234   |26   |208131.82313641932|\n",
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+------+-----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "balanced_df.unpersist()\n",
        "#featured_df.cache().display()\n",
        "\n",
        "# (Replace Databrick) - Show the resulting DataFrame\n",
        "featured_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mE0QU_59qCw"
      },
      "source": [
        "####Encode Categorical Variables:\n",
        "\n",
        "Converted categorical columns into numerical indices using StringIndexer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQJ-gODW9pTQ"
      },
      "outputs": [],
      "source": [
        "currency_index = StringIndexer(inputCol=\"Receiving_Currency\", outputCol=\"CurrencyIndex\")\n",
        "payment_format_index = StringIndexer(inputCol=\"Payment_Format\", outputCol=\"PaymentFormatIndex\")\n",
        "pattern_type_index = StringIndexer(inputCol=\"Pattern_Type\", outputCol=\"PatternTypeIndex\")\n",
        "\n",
        "featured_df = currency_index.fit(featured_df).transform(featured_df)\n",
        "featured_df = payment_format_index.fit(featured_df).transform(featured_df)\n",
        "featured_df = pattern_type_index.fit(featured_df).transform(featured_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8koyKtG9uZo",
        "outputId": "924828c6-5cc6-406f-96c2-8811a5134748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+------+-----+------------------+-------------+------------------+----------------+\n",
            "|Timestamp          |From_Bank|From_Account|To_Bank|To_Account|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Pattern_Type|isLaundering|Hour|DayOfWeek|FanOut|FanIn|AvgAmountSent     |CurrencyIndex|PaymentFormatIndex|PatternTypeIndex|\n",
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+------+-----+------------------+-------------+------------------+----------------+\n",
            "|2022-09-01 05:21:00|004      |800060A20   |004    |800060A20 |1834.72        |Rupee             |1834.72    |Rupee           |Reinvestment  |Unknown     |0           |5   |5        |234   |43   |208131.82313641932|5.0          |4.0               |0.0             |\n",
            "|2022-09-01 19:24:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |19  |5        |234   |26   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-01 05:46:00|004      |800060A20   |004    |800060A20 |10683.91       |Rupee             |10683.91   |Rupee           |Credit Card   |Unknown     |0           |5   |5        |234   |43   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-01 19:09:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |19  |5        |234   |26   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-01 05:53:00|004      |800060A20   |004    |800060A20 |11860.6        |Rupee             |11860.6    |Rupee           |Wire          |Unknown     |0           |5   |5        |234   |43   |208131.82313641932|5.0          |5.0               |0.0             |\n",
            "|2022-09-06 01:02:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |1   |3        |234   |26   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-02 07:04:00|004      |800060A20   |004    |800060A20 |21381.18       |Rupee             |21381.18   |Rupee           |Cheque        |Unknown     |0           |7   |6        |234   |43   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-06 01:25:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |1   |3        |234   |26   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-02 07:16:00|004      |800060A20   |004    |800060A20 |10683.91       |Rupee             |10683.91   |Rupee           |Credit Card   |Unknown     |0           |7   |6        |234   |43   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-08 18:49:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |18  |5        |234   |26   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-02 07:18:00|004      |800060A20   |004    |800060A20 |11860.6        |Rupee             |11860.6    |Rupee           |Wire          |Unknown     |0           |7   |6        |234   |43   |208131.82313641932|5.0          |5.0               |0.0             |\n",
            "|2022-09-08 18:31:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |18  |5        |234   |26   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-05 19:53:00|004      |800060A20   |004    |800060A20 |3117.27        |Euro              |268273.75  |Rupee           |ACH           |STACK       |1           |19  |2        |234   |43   |208131.82313641932|1.0          |2.0               |1.0             |\n",
            "|2022-09-09 14:26:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |14  |6        |234   |26   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-15 15:17:00|004      |800060A20   |004    |800060A20 |10683.91       |Rupee             |10683.91   |Rupee           |Credit Card   |Unknown     |0           |15  |5        |234   |43   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-12 12:22:00|004      |800060A20   |026    |8127B26B0 |40349.04       |Rupee             |40349.04   |Rupee           |Cheque        |Unknown     |0           |12  |2        |234   |26   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-15 15:24:00|004      |800060A20   |004    |800060A20 |11860.6        |Rupee             |11860.6    |Rupee           |Wire          |Unknown     |0           |15  |5        |234   |43   |208131.82313641932|5.0          |5.0               |0.0             |\n",
            "|2022-09-12 12:27:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |12  |2        |234   |26   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "|2022-09-05 12:43:00|004      |800060A20   |004    |800060A20 |21381.18       |Rupee             |21381.18   |Rupee           |Cheque        |Unknown     |0           |12  |2        |234   |43   |208131.82313641932|5.0          |0.0               |0.0             |\n",
            "|2022-09-13 05:06:00|004      |800060A20   |026    |8127B26B0 |40463.1        |Rupee             |40463.1    |Rupee           |Credit Card   |Unknown     |0           |5   |3        |234   |26   |208131.82313641932|5.0          |1.0               |0.0             |\n",
            "+-------------------+---------+------------+-------+----------+---------------+------------------+-----------+----------------+--------------+------------+------------+----+---------+------+-----+------------------+-------------+------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#featured_df.display()\n",
        "\n",
        "# (Replace Databrick) - Show the resulting DataFrame\n",
        "featured_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s6F1k889wNz"
      },
      "outputs": [],
      "source": [
        "# DA ERROR\n",
        "#size_in_memory_gb = featured_df.rdd.map(lambda row: len(str(row))).sum() / (1024 * 1024 * 1024)\n",
        "#print(f\"Approximate size in memory: ({size_in_memory_gb:.2f} GB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2FIO6-H9zAG"
      },
      "source": [
        "### SMOTE to generate synthetic data for minority\n",
        "\n",
        "### Compilado hasta aca la sgte da errores ########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6gyOuZP90y7",
        "outputId": "8714f889-2ec0-4a8c-80c5-7e26f145c40a"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o677.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 101.0 failed 1 times, most recent failure: Lost task 3.0 in stage 101.0 (TID 956) (host.docker.internal executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[64], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m balanced_featured_df \u001b[38;5;241m=\u001b[39m majority_df\u001b[38;5;241m.\u001b[39munion(synthetic_df)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Display counts to confirm balancing\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mbalanced_featured_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43misLaundering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o677.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 101.0 failed 1 times, most recent failure: Lost task 3.0 in stage 101.0 (TID 956) (host.docker.internal executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 49650)\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
            "    length = stream.read(4)\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Select only the required columns (excluding 'isLaundering')\n",
        "feature_columns = [\"Amount_Received\", \"FanOut\", \"FanIn\", \"AvgAmountSent\",\n",
        "                   \"Hour\", \"DayOfWeek\", \"CurrencyIndex\",\n",
        "                   \"PaymentFormatIndex\", \"PatternTypeIndex\"]\n",
        "\n",
        "# Select features for minority class (isLaundering = 1)\n",
        "minority_df = featured_df.filter(F.col(\"isLaundering\") == 1).select(*feature_columns)\n",
        "majority_df = featured_df.filter(F.col(\"isLaundering\") == 0).select(*feature_columns, \"isLaundering\")\n",
        "\n",
        "featured_df.unpersist()\n",
        "\n",
        "# Step 2: Define a function to generate synthetic samples (excluding isLaundering)\n",
        "def generate_synthetic_samples(minority_data, num_samples=10):\n",
        "    synthetic_samples = []\n",
        "\n",
        "    for row in minority_data:\n",
        "        base_vector = np.array([row[col] for col in feature_columns])\n",
        "\n",
        "        # Find random neighbors within the minority class\n",
        "        neighbors = random.sample(minority_data, k=num_samples)\n",
        "        for neighbor in neighbors:\n",
        "            neighbor_vector = np.array([neighbor[col] for col in feature_columns])\n",
        "\n",
        "            # Interpolate to create a synthetic sample\n",
        "            gap = np.random.rand()\n",
        "            synthetic_vector = base_vector + gap * (neighbor_vector - base_vector)\n",
        "\n",
        "            # Append the synthetic sample without the 'isLaundering' column\n",
        "            synthetic_samples.append(tuple(synthetic_vector.tolist()))\n",
        "\n",
        "    return synthetic_samples\n",
        "\n",
        "# Step 3: Collect minority samples and generate synthetic samples\n",
        "minority_data = minority_df.collect()\n",
        "synthetic_samples = generate_synthetic_samples(minority_data, num_samples=50)\n",
        "\n",
        "# Step 4: Define schema for synthetic samples without 'isLaundering'\n",
        "schema = StructType([StructField(col, DoubleType(), True) for col in feature_columns])\n",
        "\n",
        "# Create synthetic DataFrame from synthetic samples\n",
        "synthetic_df = spark.createDataFrame(synthetic_samples, schema=schema)\n",
        "\n",
        "# Step 5: Add 'isLaundering' column with value 1 to synthetic samples\n",
        "synthetic_df = synthetic_df.withColumn(\"isLaundering\", F.lit(1))\n",
        "\n",
        "# Step 6: Combine the majority and synthetic DataFrames\n",
        "balanced_featured_df = majority_df.union(synthetic_df)\n",
        "\n",
        "# Display counts to confirm balancing\n",
        "balanced_featured_df.cache().groupBy(\"isLaundering\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kjIQ4UJ96cD"
      },
      "outputs": [],
      "source": [
        "balanced_featured_df.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YqcpwDP98e6"
      },
      "outputs": [],
      "source": [
        "balanced_featured_df.groupBy(\"isLaundering\").count().display()\n",
        "#increased minority to 12%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJs88pQ19_Uy"
      },
      "source": [
        "##### Assemble Features into a Vector:\n",
        "\n",
        "Use VectorAssembler to create a feature vector for model input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqhOhTXj-Aie"
      },
      "outputs": [],
      "source": [
        "features_col = [\"Amount_Received\", \"FanOut\", \"FanIn\", \"AvgAmountSent\", \"Hour\", \"DayOfWeek\", \"CurrencyIndex\", \"PaymentFormatIndex\", \"PatternTypeIndex\"]\n",
        "correlation_features = [\"Amount_Received\", \"FanOut\", \"FanIn\", \"AvgAmountSent\", \"Hour\", \"DayOfWeek\", \"CurrencyIndex\", \"PaymentFormatIndex\", \"PatternTypeIndex\", \"isLaundering\"]\n",
        "assembler = VectorAssembler(inputCols=correlation_features, outputCol=\"corr_features\")\n",
        "df_vector = assembler.transform(balanced_featured_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSY582zD-Dvr"
      },
      "source": [
        "\n",
        "##### Correlation Heatmap: To assess relationships between features and identify strong predictors,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7vIzqgy-CST"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = Correlation.corr(df_vector, \"corr_features\").head()[0].toArray()\n",
        "\n",
        "# Convert to a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, xticklabels=correlation_features, yticklabels=correlation_features, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dt65PYQ-fGS"
      },
      "source": [
        "### Observations:\n",
        "\n",
        "###### Correlation Heatmap:\n",
        "\n",
        "Amount_Received and AvgAmountSent show a moderately positive correlation (0.34). Including both might introduce some redundancy, but they may still capture distinct aspects of the data.\n",
        "\n",
        "Other features have relatively low correlations with each other, which generally indicates that they might add unique information to the model.\n",
        "\n",
        "- PatternTypeIndex Has the strongest positive correlation with\n",
        "isLaundering (0.89).\n",
        "- CurrencyIndex Shows a weak positive correlation (0.046) with isLaundering.\n",
        "- PaymentFormatIndex Also shows a weak positive correlation (0.053) with isLaundering.\n",
        "- FanOut and DayOfWeek Show negative correlations (-0.087 and -0.067, respectively) with isLaundering.\n",
        "- Hour and AvgAmountSent Show weak positive correlations (0.014 and 0.053, respectively) with isLaundering.\n",
        "- Amount_Received Has an extremely weak positive correlation (0.0059) with isLaundering, suggesting it's nearly uncorrelated.\n",
        "- FanIn Has a weak negative correlation (-0.01) with isLaundering.\n",
        "#### Key Takeaways:\n",
        "- PatternTypeIndex is the standout feature, showing a strong correlation with isLaundering. Its critical to include this in the model.\n",
        "- Features like CurrencyIndex and PaymentFormatIndex have weak correlations but could still contribute useful information when combined with others.\n",
        "- Features like Amount_Received, FanIn, and Hour have very weak correlations, and their inclusion should be carefully considered to avoid adding noise to the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgedS_Hm_POC"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME3tGbVZ-Om6"
      },
      "outputs": [],
      "source": [
        "# Define all features for the final assembler, using scaled and unscaled features\n",
        "all_features = [ \"FanOut\", \"AvgAmountSent\", \"DayOfWeek\", \"CurrencyIndex\", \"PaymentFormatIndex\", \"PatternTypeIndex\"]\n",
        "\n",
        "# Assemble the final feature vector for prediction\n",
        "assembler_final = VectorAssembler(inputCols=all_features, outputCol=\"features\")\n",
        "balanced_featured_df = balanced_featured_df.select(all_features + [\"isLaundering\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEsvvcKi_T-u"
      },
      "outputs": [],
      "source": [
        "size_in_memory_gb = balanced_featured_df.rdd.map(lambda row: len(str(row))).sum() / (1024 * 1024 * 1024)\n",
        "print(f\"Approximate size in memory: ({size_in_memory_gb:.2f} GB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM9uES0r_WE6"
      },
      "source": [
        "#####Split the Data:\n",
        "\n",
        "Split the data into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynKdgyBQ_U87"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, test_df = balanced_featured_df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
        "balanced_featured_df.unpersist()\n",
        "train_df.cache()\n",
        "val_df.cache()\n",
        "test_df.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCY-yUeLAzMy"
      },
      "source": [
        "##Model Training with Pipeline\n",
        "\n",
        "#####Created a Pipeline with Random Forest Classifier ML model:\n",
        "\n",
        "Defined a pipeline to streamline the feature transformations and model training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Cq0LV63A0Od"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"isLaundering\", numTrees=20, maxDepth=10)\n",
        "pipeline = Pipeline(stages=[ assembler_final, rf])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrMQv5sUA4lm"
      },
      "source": [
        "#####Hyperparameter Tuning using CrossValidator:\n",
        "\n",
        "-Used CrossValidator to find the best hyperparameters for the Random Forest model.\n",
        "\n",
        "-BinaryClassificationEvaluator Used to evaluate the model's performance.\n",
        "\n",
        "-areaUnderROC is a common metric for binary classification, especially useful for imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv2YO01PA2dr"
      },
      "outputs": [],
      "source": [
        "paramGridSearch = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).addGrid(rf.maxDepth, [10]).build()\n",
        "evaluatorr = BinaryClassificationEvaluator(labelCol=\"isLaundering\", metricName=\"areaUnderROC\")\n",
        "crossvalidation = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGridSearch, evaluator=evaluatorr, numFolds=3)\n",
        "cvModel = crossvalidation.fit(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LENszS8pBDdx"
      },
      "source": [
        "\n",
        "##Evaluate the Model\n",
        "\n",
        "#####Evaluate on Validation Data:\n",
        "\n",
        "Used F1 Score, Area Under ROC metrics to evaluate the model on validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1ZTaHogA_Te"
      },
      "outputs": [],
      "source": [
        "# Evaluate ROC on validation data\n",
        "predictions = cvModel.transform(val_df)\n",
        "roc_score = evaluatorr.evaluate(predictions)\n",
        "print(\"Area Under ROC Score on validation data:\", roc_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue6jWEaiBKJ7"
      },
      "outputs": [],
      "source": [
        "# Evaluate F1 Score on validation data\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"isLaundering\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(\"F1 Score on validation data:\", f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNlP49gYBLd0"
      },
      "source": [
        "\n",
        "#####Final Testing:\n",
        "\n",
        "Test the model on the test set to get final performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tNUY2EZBNbN"
      },
      "outputs": [],
      "source": [
        "# Evaluate ROC on test data\n",
        "final_predictions = cvModel.transform(test_df)\n",
        "test_roc_score = evaluatorr.evaluate(final_predictions)\n",
        "print(\"Final ROC Score on test data:\", test_roc_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Eof7qmBBPdJ"
      },
      "outputs": [],
      "source": [
        "# Evaluate F1 Score on test data\n",
        "test_f1_score = evaluator_f1.evaluate(final_predictions)\n",
        "print(\"Final F1 Score on test data:\", test_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCeqq8pgBTGa"
      },
      "outputs": [],
      "source": [
        "#evaluate percision and recall\n",
        "precision_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"isLaundering\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"isLaundering\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "\n",
        "precision = precision_evaluator.evaluate(final_predictions)\n",
        "recall = recall_evaluator.evaluate(final_predictions)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoK-vHjoBjsR"
      },
      "source": [
        "### Observations\n",
        "Unrealistically Good Performance, can be caused by\n",
        "\n",
        "####- Data Leakage:\n",
        "If features or information from the test data inadvertently influenced the training process, the model could appear to perform perfectly. Like Using features directly derived from the target variable. (PatternTypeIndex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofAa_1RBrTa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}